{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datenaufbereitung",
   "id": "3a9152ae6407a5a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### stocks_import.py",
   "id": "8819d81e1e5ea8f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "# Zeitraum: letzte 30 Tage\n",
    "today = datetime.date.today()\n",
    "start_date = today - datetime.timedelta(days=60)\n",
    "end_date = today\n",
    "\n",
    "# Aktienkurse (täglich)\n",
    "tickers = [\"NVDA\", \"GOOG\", \"MSFT\"]\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=True)\n",
    "    df.to_csv(f\"../../03_Daten/raw_data/historical_stock_data_daily_{ticker}_last60d.csv\")\n",
    "    time.sleep(10)  # 10 Sekunden warten"
   ],
   "id": "f9aef959144577f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### data_format.py",
   "id": "dca0aceadb91906c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "ticker = \"GOOG\"\n",
    "csv_path = f\"../../03_Daten/raw_data/historical_stock_data_daily_{ticker}_last60d.csv\"\n",
    "output_path = f\"../../03_Daten/processed_data/historical_stock_data_daily_{ticker}_last60d_flat.csv\"\n",
    "\n",
    "# 1) Einlesen mit Angabe, dass 2 Headerzeilen vorhanden sind und der Index die \"Date\"-Spalte ist\n",
    "df = pd.read_csv(csv_path, header=[0, 1], index_col=0)\n",
    "print(\"Vor Flatten:\\n\", df.head())\n",
    "print(\"\\nMultiIndex Columns:\\n\", df.columns)\n",
    "\n",
    "# 2) Entferne die Ticker-Ebene (Level 1), sodass nur die Spaltennamen übrig bleiben\n",
    "df.columns = df.columns.droplevel(1)\n",
    "print(\"\\nSpalten nach droplevel(1):\\n\", df.columns)\n",
    "\n",
    "# 3) Falls das Datum aktuell als Index vorliegt, diesen zurück in eine eigene Spalte holen\n",
    "df.reset_index(inplace=True)\n",
    "# Falls der Indexname nicht bereits \"Date\" ist, kann man ihn umbenennen\n",
    "if df.columns[0] != \"Date\":\n",
    "    df.rename(columns={df.columns[0]: \"Date\"}, inplace=True)\n",
    "\n",
    "# 4) Wähle explizit die gewünschten Spalten in der gewünschten Reihenfolge\n",
    "df = df[[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]]\n",
    "\n",
    "# 5) Datum in ein Datetime-Format umwandeln (optional, aber empfohlen)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "# 6) Das geflattete DataFrame als neue CSV speichern\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"\\nNach Flatten:\\n\", df.head())\n",
    "print(f\"\\nFlattened CSV gespeichert unter: {output_path}\")\n"
   ],
   "id": "8db1eb938922a9e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### gtd_import.py",
   "id": "c62e3f8df81c72ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-05-14\"\n",
    "timeframe_str = f\"{start_date} {end_date}\"\n",
    "\n",
    "ticker_keywords = {\n",
    "    \"NVDA\": [\"NVIDIA stock\", \"sell NVIDIA stock\", \"buy NVIDIA stock\"],\n",
    "    \"GOOG\": [\"Google stock\", \"sell Google stock\", \"buy Google stock\"],\n",
    "    \"MSFT\": [\"Microsoft stock\", \"sell Microsoft stock\", \"buy Microsoft stock\"]\n",
    "}\n",
    "\n",
    "# Erstellen einer pytrends-Instanz\n",
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "\n",
    "# Für jede Aktie: Abrufen der Google Trends Daten für alle Suchbegriffe und Zusammenführen der Ergebnisse\n",
    "for ticker, keywords in ticker_keywords.items():\n",
    "    print(f\"\\nStarte Abruf der Google Trends Daten für {ticker}...\")\n",
    "\n",
    "    df_all = None  # DataFrame, in dem die Zeitreihen-Daten aller Keywords gespeichert werden\n",
    "    for keyword in keywords:\n",
    "        print(f\"  Abrufe Keyword: {keyword}\")\n",
    "        kw_list = [keyword]\n",
    "        pytrends.build_payload(kw_list, cat=0, timeframe=timeframe_str, geo='', gprop='')\n",
    "\n",
    "        # Retry-Logik mit exponentiellem Backoff\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        wait_time = 60  # Start-Wartezeit in Sekunden\n",
    "\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                trends_data = pytrends.interest_over_time()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(\n",
    "                    f\"    Fehler beim Abruf von '{keyword}': {e}. Warte {wait_time} Sekunden, Versuch {retry_count} von {max_retries}...\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                wait_time *= 2  # Wartezeit verdoppeln\n",
    "        else:\n",
    "            raise Exception(f\"Mehrere Versuche für '{keyword}' fehlgeschlagen. Bitte überprüfen Sie Ihre Anfrage.\")\n",
    "\n",
    "        # Entfernen der 'isPartial'-Spalte, falls vorhanden\n",
    "        if 'isPartial' in trends_data.columns:\n",
    "            trends_data = trends_data.drop(columns=['isPartial'])\n",
    "\n",
    "        # Umbenennen der Spalte in den Keyword-Namen\n",
    "        trends_data = trends_data.rename(columns={keyword: f\"{keyword}\"})\n",
    "\n",
    "        # Falls df_all noch nicht existiert: setze es auf den aktuellen DataFrame\n",
    "        if df_all is None:\n",
    "            df_all = trends_data[[f\"{keyword}\"]].copy()\n",
    "        else:\n",
    "            # Merge: Zusammenführen an der Datumsspalte (Index)\n",
    "            df_all = df_all.merge(trends_data[[f\"{keyword}\"]], left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    # Ergebnisse chronologisch sortieren\n",
    "    df_all = df_all.sort_index()\n",
    "    print(f\"\\nErste Zeilen der kombinierten Google Trends Daten für {ticker}:\")\n",
    "    print(df_all.head())\n",
    "\n",
    "    # kombiniertes DataFrame als CSV speichern\n",
    "    output_filename = f\"../../03_Daten/raw_data/google_trends_daily_{ticker}_last30d.csv\"\n",
    "    df_all.to_csv(output_filename)\n",
    "    print(f\"Google Trends Daten für {ticker} wurden in '{output_filename}' gespeichert.\")\n"
   ],
   "id": "71c34b1d534a6abd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### gtd_merger.py",
   "id": "8f22276ee7281206"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Einstellungen\n",
    "tickers = [\"NVDA\", \"GOOG\", \"MSFT\"]\n",
    "periods = [\"2015-2020\", \"2020-2023\", \"2023-2025\"]\n",
    "input_pattern = \"../../03_Daten/raw_data/google_trends_weekly_{ticker}_{period}.csv\"\n",
    "output_dir = \"../../03_Daten/processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Liste zum Sammeln der DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # 1) Die CSVs für die 3 Perioden einlesen\n",
    "    for period in periods:\n",
    "        fn = input_pattern.format(ticker=ticker, period=period)\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                fn,\n",
    "                parse_dates=[\"date\"],\n",
    "                index_col=\"date\"\n",
    "            )\n",
    "            dfs.append(df)\n",
    "            print(f\"eingelesen: {fn} ({len(df)} Zeilen)\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Datei nicht gefunden: {fn}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"Keine Daten für {ticker}, überspringe.\")\n",
    "        continue\n",
    "\n",
    "    # 2) Aneinanderhängen\n",
    "    merged = pd.concat(dfs)\n",
    "\n",
    "    # 3) Nach Datum sortieren\n",
    "    merged = merged.sort_index()\n",
    "\n",
    "    # 4) Duplikate (gleicher Index) entfernen, ersten behalten\n",
    "    merged = merged[~merged.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    # 5) Abspeichern\n",
    "    out_fn = os.path.join(\n",
    "        output_dir,\n",
    "        f\"google_trends_weekly_{ticker}_2015-2025.csv\"\n",
    "    )\n",
    "    merged.to_csv(out_fn)\n",
    "    print(f\"gespeichert: {out_fn} ({len(merged)} Zeilen)\\n\")\n"
   ],
   "id": "c5a13791e2afa05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### data_merger.py",
   "id": "7748ac6be17a1763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Liste der Ticker, für die die Daten zusammengeführt werden sollen\n",
    "tickers = [\"NVDA\", \"GOOG\", \"MSFT\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    # 1) Aktienkurs- & RSI-Daten einlesen, sortiert nach Datum\n",
    "    stock_path = f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat_with_RSI.csv\"\n",
    "    stock_df = (\n",
    "        pd.read_csv(stock_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "          .sort_index()\n",
    "    )\n",
    "\n",
    "    # 2) Google Trends-Daten einlesen, sortiert\n",
    "    trends_path = f\"../../03_Daten/processed_data/google_trends_weekly_{ticker}_2015-2025.csv\"\n",
    "    trends_df = (\n",
    "        pd.read_csv(trends_path, parse_dates=[\"date\"], index_col=\"date\")\n",
    "          .sort_index()\n",
    "    )\n",
    "\n",
    "    # 3) Gemeinsamen Zeithorizont bestimmen (ca. 10 Jahre)\n",
    "    start_date = max(stock_df.index.min(), trends_df.index.min())\n",
    "    end_date   = min(stock_df.index.max(), trends_df.index.max())\n",
    "    stock_df   = stock_df.loc[start_date:end_date]\n",
    "    trends_df  = trends_df.loc[start_date:end_date]\n",
    "\n",
    "    # 4) Auf Wochenperioden abbilden\n",
    "    stock_df.index  = stock_df.index.to_period(\"W\")\n",
    "    trends_df.index = trends_df.index.to_period(\"W\")\n",
    "\n",
    "    # 5) Inner Join auf Wochenniveau\n",
    "    merged = stock_df.join(\n",
    "        trends_df,\n",
    "        how=\"inner\",\n",
    "        lsuffix=\"\",\n",
    "        rsuffix=\"_trend\"\n",
    "    )\n",
    "\n",
    "    # 6) PeriodIndex zurück in Timestamps\n",
    "    merged.index = merged.index.to_timestamp()\n",
    "\n",
    "    # 7) Abspeichern\n",
    "    out_path = f\"../../03_Daten/processed_data/merged_weekly_{ticker}_2015-2025.csv\"\n",
    "    merged.to_csv(out_path)\n",
    "    print(f\"{ticker}: Merged über {merged.shape[0]} Wochen → {out_path}\")\n"
   ],
   "id": "b362141b979e3d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Featureengineering",
   "id": "b50729c22690cb21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RSI_feature.py",
   "id": "e6dead421bb8b77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_rsi(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Berechnet den klassischen 14‑Perioden‑RSI mit min_periods,\n",
    "    sodass erst ab Index = period Werte ausgegeben werden.\n",
    "    \"\"\"\n",
    "    delta = df[\"Close\"].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    # hier min_periods=period setzen\n",
    "    ema_up = up.ewm(com=period-1, adjust=False, min_periods=period).mean()\n",
    "    ema_down = down.ewm(com=period-1, adjust=False, min_periods=period).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Pfad zur geflatteten CSV\n",
    "ticker = \"MSFT\"\n",
    "infile = f\"../../03_Daten/processed_data/historical_stock_data_daily_{ticker}_last60d_flat.csv\"\n",
    "outfile = f\"../../03_Daten/processed_data/historical_stock_data_daily_{ticker}_last60d_flat_with_RSI.csv\"\n",
    "\n",
    "# CSV einlesen\n",
    "df = pd.read_csv(infile, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# RSI berechnen (erst ab der 14. Woche echte Werte)\n",
    "df[\"RSI_14\"] = compute_rsi(df, period=14)\n",
    "\n",
    "# Kurzcheck auf NaNs in den ersten Reihen\n",
    "print(df[[\"Close\", \"RSI_14\"]].head(20))\n",
    "\n",
    "# Neue CSV speichern\n",
    "df.to_csv(outfile, index=True)\n",
    "print(f\"RSI hinzugefügt und gespeichert in {outfile}\")\n"
   ],
   "id": "10c47b59fedea4fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementierung der Vergleichsmodelle",
   "id": "9c918849deda58d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ARIMA.py",
   "id": "bc877333119862ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "tickers = [\"NVDA\", \"GOOG\", \"MSFT\"]\n",
    "results_arima = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(\n",
    "        f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "        parse_dates=['Date'], index_col='Date'\n",
    "    ).sort_index()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    fold = 1\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        train, test = df.iloc[train_index], df.iloc[test_index]\n",
    "        model = ARIMA(train['Close'], order=(1,1,1)).fit()\n",
    "        forecast = model.forecast(steps=len(test))\n",
    "        forecast.index = test.index\n",
    "\n",
    "        # metrics\n",
    "        y_true = test['Close']\n",
    "        y_pred = forecast\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae  = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "        results_arima.append({\n",
    "            'Ticker': ticker,\n",
    "            'Fold': fold,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        })\n",
    "\n",
    "        plt.plot(test.index, test['Close'], label=f'Fold {fold} Actual', linewidth=2)\n",
    "        plt.plot(forecast.index, forecast, linestyle='--', label=f'Fold {fold} Forecast', linewidth=2)\n",
    "        fold += 1\n",
    "\n",
    "    plt.title(f\"{ticker} ARIMA(1,1,1) Forecasts across 3 Folds\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Zusammenfassung der Einzel-Fold-Metriken\n",
    "summary_df = pd.DataFrame(results_arima)\n",
    "print(\"Einzel-Fold Metriken:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Durchschnittliche Metriken über alle 3 Folds je Ticker\n",
    "avg_metrics = summary_df.groupby('Ticker').agg(\n",
    "    RMSE=('RMSE','mean'),\n",
    "    MAE=('MAE','mean'),\n",
    "    MAPE=('MAPE','mean'),\n",
    "    R2=('R2','mean')\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nDurchschnittliche Metriken je Ticker über 3 Folds:\")\n",
    "print(avg_metrics)\n"
   ],
   "id": "ec47e05b2cec282d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SARIMAX.py",
   "id": "45676eca29862d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# Ticker-Liste und Ergebnis-Speicher\n",
    "tickers = [\"NVDA\", \"GOOG\", \"MSFT\"]\n",
    "results_sarimax = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Daten einlesen\n",
    "    df = pd.read_csv(\n",
    "        f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "        parse_dates=[\"Date\"],\n",
    "        index_col=\"Date\"\n",
    "    ).sort_index()\n",
    "\n",
    "    # TimeSeriesSplit-Validierung\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    fold = 1\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for train_idx, test_idx in tscv.split(df):\n",
    "        train = df.iloc[train_idx]\n",
    "        test  = df.iloc[test_idx]\n",
    "\n",
    "        model = SARIMAX(\n",
    "            train['Close'],\n",
    "            order=(1, 1, 1),\n",
    "            seasonal_order=(1, 1, 1, 52),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        fit = model.fit(disp=False)\n",
    "\n",
    "        # Forecast für Test\n",
    "        forecast = fit.forecast(steps=len(test))\n",
    "        forecast.index = test.index\n",
    "\n",
    "        # Metriken berechnen\n",
    "        y_true = test['Close']\n",
    "        y_pred = forecast\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae  = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "        results_sarimax.append({\n",
    "            'Ticker': ticker,\n",
    "            'Fold': fold,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        })\n",
    "\n",
    "        # Plot Actual vs Forecast\n",
    "        plt.plot(test.index, y_true, label=f'Fold {fold} Actual', linewidth=2)\n",
    "        plt.plot(forecast.index, forecast, linestyle='--', label=f'Fold {fold} Forecast', linewidth=2)\n",
    "        fold += 1\n",
    "\n",
    "    plt.title(f\"{ticker} SARIMAX(1,1,1)x(1,1,1,52) Forecasts across 3 Folds\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Zusammenfassung der Fold-Metriken\n",
    "summary_df = pd.DataFrame(results_sarimax)\n",
    "print(\"Einzel-Fold SARIMAX Metriken:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Durchschnittliche Metriken je Ticker\n",
    "avg_metrics = summary_df.groupby('Ticker').agg(\n",
    "    RMSE=('RMSE','mean'),\n",
    "    MAE=('MAE','mean'),\n",
    "    MAPE=('MAPE','mean'),\n",
    "    R2=('R2','mean')\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nDurchschnittliche SARIMAX Metriken je Ticker über 3 Folds:\")\n",
    "print(avg_metrics)\n"
   ],
   "id": "1ef6b2bc0946fc46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### lstm.py",
   "id": "974729042de50ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "\n",
    "# 1) Daten einlesen und vorbereiten\n",
    "ticker = \"MSFT\"\n",
    "df = pd.read_csv(\n",
    "    f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "    parse_dates=[\"Date\"], index_col=\"Date\"\n",
    ").sort_index()\n",
    "data = df[[\"Close\"]].copy()\n",
    "\n",
    "# 2) Skalierung in [0,1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# 3) Funktion, um Sequenzen zu bauen\n",
    "def create_sequences(dataset, window_size=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i : i + window_size, 0])\n",
    "        y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# 4) TimeSeriesSplit\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Storage\n",
    "rmse_list, mae_list, mape_list, r2_list, hit_rate_list, sharpe_list = ([] for _ in range(6))\n",
    "all_y_true, all_y_pred, all_dates = [], [], []\n",
    "\n",
    "# 5) Loop über Folds\n",
    "fold_num = 1\n",
    "for train_idx, test_idx in tscv.split(scaled_data):\n",
    "    # Split in train/test\n",
    "    train_data = scaled_data[train_idx]\n",
    "    test_data  = scaled_data[test_idx]\n",
    "\n",
    "    # Sequenzen\n",
    "    X_train, y_train = create_sequences(train_data, window_size)\n",
    "    X_test,  y_test  = create_sequences(test_data,  window_size)\n",
    "\n",
    "    # Reshape für LSTM\n",
    "    X_train = X_train.reshape((-1, window_size, 1))\n",
    "    X_test  = X_test.reshape((-1, window_size, 1))\n",
    "\n",
    "    # Modell definieren\n",
    "    model = Sequential([\n",
    "        Input(shape=(window_size, 1)),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "\n",
    "    # @tf.function Inferenz\n",
    "    @tf.function\n",
    "    def predict_fn(x):\n",
    "        return model(x, training=False)\n",
    "\n",
    "    # Vorhersage & inverse Skalierung\n",
    "    y_pred_scaled = predict_fn(tf.constant(X_test)).numpy().reshape(-1, 1)\n",
    "    y_test_scaled = y_test.reshape(-1, 1)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "    y_true = scaler.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "    # Datums-Indizes für diesen Fold (erste window_size Zeitpunkte entfallen)\n",
    "    dates = df.index[test_idx][window_size:]\n",
    "    all_dates.append(dates)\n",
    "\n",
    "    # Metriken berechnen\n",
    "    rmse  = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae   = mean_absolute_error(y_true, y_pred)\n",
    "    mape  = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2    = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Speichern\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    mape_list.append(mape)\n",
    "    r2_list.append(r2)\n",
    "    all_y_true.append(y_true)\n",
    "    all_y_pred.append(y_pred)\n",
    "\n",
    "    # Konsolenausgabe\n",
    "    print(f\"Fold {fold_num} – RMSE={rmse:.4f}, MAE={mae:.4f}, \"\n",
    "          f\"MAPE={mape:.2f}%, R²={r2:.4f}\")\n",
    "    fold_num += 1\n",
    "\n",
    "# 6) Durchschnitt aller Folds\n",
    "print(\"\\n=== Durchschnitt aller Folds ===\")\n",
    "print(f\"RMSE:     {np.mean(rmse_list):.4f}\")\n",
    "print(f\"MAE:      {np.mean(mae_list):.4f}\")\n",
    "print(f\"MAPE:     {np.mean(mape_list):.4f}%\")\n",
    "print(f\"R²:       {np.mean(r2_list):.4f}\")\n",
    "\n",
    "# 7) Ein einziger Plot für alle 3 Folds\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(n_splits):\n",
    "    # Real\n",
    "    plt.plot(all_dates[i], all_y_true[i], label=f\"Real Prc Fold {i+1}\")\n",
    "    # Pred (gestrichelt)\n",
    "    plt.plot(all_dates[i], all_y_pred[i], linestyle=\"--\", label=f\"Pred Prc Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Aktienkurse alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2798e976ad621adf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGBoost.py",
   "id": "b3b70df037b0113c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1) CSV laden und Daten vorbereiten\n",
    "ticker = \"NVDA\"\n",
    "df = pd.read_csv(\n",
    "    f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "    parse_dates=[\"Date\"], index_col=\"Date\"\n",
    ").sort_index()\n",
    "data = df[[\"Close\"]].copy()\n",
    "values = data.values\n",
    "\n",
    "# 2) Funktion zum Erstellen von Sequenzen (Windowing)\n",
    "def create_sequences(dataset, window_size=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        X.append(dataset[i : i + window_size, 0])\n",
    "        y.append(dataset[i + window_size, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# 3) TimeSeriesSplit einrichten\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# 4) Storage für Metriken und Kursverläufe\n",
    "rmse_list, mae_list, mape_list, r2_list = [], [], [], []\n",
    "hit_rate_list, sharpe_list = [], []\n",
    "all_y_true, all_y_pred, all_dates = [], [], []\n",
    "\n",
    "# 5) Loop über Folds\n",
    "fold_num = 1\n",
    "for train_idx, test_idx in tscv.split(values):\n",
    "    # a) Train/Test-Split\n",
    "    train_data = values[train_idx]\n",
    "    test_data  = values[test_idx]\n",
    "\n",
    "    # b) Sequenzen erstellen\n",
    "    X_train, y_train = create_sequences(train_data, window_size)\n",
    "    X_test,  y_test  = create_sequences(test_data,  window_size)\n",
    "\n",
    "    print(f\"Fold {fold_num}: X_train={X_train.shape}, y_train={y_train.shape}, \"\n",
    "          f\"X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "    # c) XGBoost definieren & trainieren\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # d) Vorhersage\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Datums-Indizes für Plot\n",
    "    dates = df.index[test_idx][window_size:]\n",
    "    all_dates.append(dates)\n",
    "\n",
    "    # Speichern der echten und prognostizierten Kurse\n",
    "    all_y_true.append(y_test)\n",
    "    all_y_pred.append(y_pred)\n",
    "\n",
    "    # e) Metriken berechnen\n",
    "    rmse  = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae   = mean_absolute_error(y_test, y_pred)\n",
    "    mape  = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2    = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Ergebnisse sammeln\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "    mape_list.append(mape)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "    # Konsolenausgabe pro Fold\n",
    "    print(f\"Fold {fold_num} – \"\n",
    "          f\"RMSE={rmse:.4f}, \"\n",
    "          f\"MAE={mae:.4f}, \"\n",
    "          f\"MAPE={mape:.2f}%, \"\n",
    "          f\"R²={r2:.4f}\")\n",
    "    fold_num += 1\n",
    "\n",
    "# 6) Durchschnitt über alle Folds\n",
    "print(\"=== Durchschnitt über alle Folds ===\")\n",
    "print(f\"RMSE:       {np.mean(rmse_list):.4f}\")\n",
    "print(f\"MAE:        {np.mean(mae_list):.4f}\")\n",
    "print(f\"MAPE:       {np.mean(mape_list):.4f}%\")\n",
    "print(f\"R²:         {np.mean(r2_list):.4f}\")\n",
    "\n",
    "# 7) Plot für alle 3 Folds\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(all_dates[i], all_y_true[i],\n",
    "             label=f\"Real Prc Fold {i+1}\")\n",
    "    plt.plot(all_dates[i], all_y_pred[i],\n",
    "             linestyle=\"--\", label=f\"Pred Prc Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Aktienkurse alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "3f3a6811e26a07f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Entwicklung und Training der Hybridmodelle",
   "id": "c5795fffbb3d38e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_LSTM2.py",
   "id": "693b0d88680b96a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
    "\n",
    "# 1) Daten einlesen & Renditen berechnen\n",
    "ticker = \"GOOG\"\n",
    "df = pd.read_csv(\n",
    "    f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "    parse_dates=[\"Date\"], index_col=\"Date\"\n",
    ").sort_index()\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 2) CV-Setup\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "window_size = 10\n",
    "\n",
    "# 3) Speicher für Metriken\n",
    "rmse_ret, mae_ret, mape_ret, r2_ret = [], [], [], []\n",
    "hit_ret, sharpe_ret = [], []\n",
    "rmse_pr, mae_pr, mape_pr, r2_pr = [], [], [], []\n",
    "hit_pr, sharpe_pr = [], []\n",
    "\n",
    "# 4) Speicher für Plots\n",
    "all_ret_true, all_ret_pred, dates_ret = [], [], []\n",
    "all_pr_true, all_pr_pred, dates_pr = [], [], []\n",
    "\n",
    "# Helper zum Feature-Engineering\n",
    "def create_features_and_target(df, window_size=10):\n",
    "    X, y = [], []\n",
    "    rets = df[\"Return\"].values\n",
    "    vol  = df[\"GARCH_vol\"].values\n",
    "    for i in range(window_size, len(df)):\n",
    "        X.append(np.concatenate([rets[i-window_size:i], [vol[i]]]))\n",
    "        y.append(rets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 5) Schleife über die Folds\n",
    "fold = 1\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    train_df = df.iloc[train_idx].copy()\n",
    "    test_df  = df.iloc[test_idx].copy()\n",
    "\n",
    "    # a) GARCH auf den Trainings-Returns fitten\n",
    "    scaled = train_df[\"Return\"] * 10\n",
    "    gm = arch_model(scaled, mean=\"Zero\", vol=\"GARCH\", p=1, q=1, dist=\"normal\", rescale=False)\n",
    "    res = gm.fit(disp=\"off\")\n",
    "    train_df[\"GARCH_vol\"] = res.conditional_volatility / 10\n",
    "\n",
    "    # b) GARCH-Forecast für den Testabschnitt\n",
    "    horizon = len(test_df)\n",
    "    fc = res.forecast(start=train_df.index[-1], horizon=horizon, reindex=False)\n",
    "    test_df[\"GARCH_vol\"] = np.sqrt(fc.variance.values[-1, :]) / 10\n",
    "\n",
    "    # c) Features und Target\n",
    "    X_tr, y_tr = create_features_and_target(train_df, window_size)\n",
    "    X_te, y_te = create_features_and_target(test_df,  window_size)\n",
    "\n",
    "    # d) LSTM definieren & trainieren\n",
    "    X_tr_l = X_tr.reshape(-1, window_size+1, 1)\n",
    "    X_te_l = X_te.reshape(-1, window_size+1, 1)\n",
    "    model = Sequential([\n",
    "        Input(shape=(window_size+1, 1)),\n",
    "        LSTM(50, return_sequences=True), Dropout(0.2),\n",
    "        LSTM(50),                     Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(\"adam\", \"mse\")\n",
    "    model.fit(X_tr_l, y_tr, epochs=20, batch_size=16, verbose=0)\n",
    "\n",
    "    # e) Vorhersage der Log-Returns\n",
    "    y_pred = model.predict(X_te_l).flatten()\n",
    "\n",
    "    # Datums-Vektor\n",
    "    dates = test_df.index[window_size:]\n",
    "    dates_ret.append(dates)\n",
    "    all_ret_true.append(y_te)\n",
    "    all_ret_pred.append(y_pred)\n",
    "\n",
    "    # f) Metriken auf Returns\n",
    "    rm = math.sqrt(mean_squared_error(y_te, y_pred))\n",
    "    ma = mean_absolute_error(y_te, y_pred)\n",
    "    denom_r = np.where(y_te == 0, np.nan, y_te)\n",
    "    mp = np.nanmean(np.abs((y_te - y_pred) / denom_r)) * 100\n",
    "    r2v = r2_score(y_te, y_pred)\n",
    "    dir_t = np.sign(np.diff(y_te))\n",
    "    dir_p = np.sign(np.diff(y_pred))\n",
    "    hr  = (dir_t == dir_p).mean() * 100\n",
    "    rets_pred = np.diff(y_pred) / y_pred[:-1]\n",
    "    sr  = rets_pred.mean() / (rets_pred.std() if rets_pred.std() != 0 else np.nan)\n",
    "\n",
    "    rmse_ret.append(rm); mae_ret.append(ma); mape_ret.append(mp)\n",
    "    r2_ret.append(r2v); hit_ret.append(hr); sharpe_ret.append(sr)\n",
    "\n",
    "    # g) Rückrechnung in Preise\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        prev_price = test_df[\"Close\"].iloc[i + window_size - 1]\n",
    "        preds.append(prev_price * np.exp(r))\n",
    "        actuals.append(test_df[\"Close\"].iloc[i + window_size])\n",
    "    preds   = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    dates_pr.append(dates)\n",
    "    all_pr_true.append(actuals)\n",
    "    all_pr_pred.append(preds)\n",
    "\n",
    "    # h) Metriken auf Preise\n",
    "    rm_p = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    ma_p = mean_absolute_error(actuals, preds)\n",
    "    denom_p = np.where(actuals == 0, np.nan, actuals)\n",
    "    mp_p = np.nanmean(np.abs((actuals - preds) / denom_p)) * 100\n",
    "    r2p  = r2_score(actuals, preds)\n",
    "    dt   = np.sign(np.diff(actuals))\n",
    "    dp   = np.sign(np.diff(preds))\n",
    "    hr_p = (dt == dp).mean() * 100\n",
    "    rets_p = np.diff(preds) / preds[:-1]\n",
    "    sr_p    = rets_p.mean() / (rets_p.std() if rets_p.std() != 0 else np.nan)\n",
    "\n",
    "    rmse_pr.append(rm_p); mae_pr.append(ma_p); mape_pr.append(mp_p)\n",
    "    r2_pr.append(r2p);  hit_pr.append(hr_p);  sharpe_pr.append(sr_p)\n",
    "\n",
    "    print(f\"Fold {fold} fertig.\")\n",
    "    fold += 1\n",
    "\n",
    "# 6) Durchschnittswerte ausgeben\n",
    "print(f\"=== Log-Returns (Ø über {n_splits} Folds) ===\")\n",
    "print(f\"RMSE: {np.mean(rmse_ret):.4f}, MAE: {np.mean(mae_ret):.4f}, \"\n",
    "      f\"MAPE: {np.mean(mape_ret):.4f}%, R²: {np.mean(r2_ret):.4f}, \"\n",
    "      f\"Hit: {np.mean(hit_ret):.4f}%, Sharpe: {np.nanmean(sharpe_ret):.4f}\")\n",
    "print(f\"=== Close-Preise (Ø über {n_splits} Folds) ===\")\n",
    "print(f\"RMSE: {np.mean(rmse_pr):.4f}, MAE: {np.mean(mae_pr):.4f}, \"\n",
    "      f\"MAPE: {np.mean(mape_pr):.4f}%, R²: {np.mean(r2_pr):.4f}, \"\n",
    "      f\"Hit: {np.mean(hit_pr):.4f}%, Sharpe: {np.nanmean(sharpe_pr):.4f}\")\n",
    "\n",
    "# 7) Log-Returns aller 3 Folds\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_ret[i], all_ret_true[i],  label=f\"Real Ret Fold {i+1}\")\n",
    "    plt.plot(dates_ret[i], all_ret_pred[i], linestyle=\"--\", label=f\"Pred Ret Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Log-Renditen alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log-Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Close-Preise aller 3 Folds\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_pr[i], all_pr_true[i],  label=f\"Real Prc Fold {i+1}\")\n",
    "    plt.plot(dates_pr[i], all_pr_pred[i], linestyle=\"--\", label=f\"Pred Prc Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Close-Preise alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1143ea1d50858c7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_LSTM_RSI2.py",
   "id": "211901282646f49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Einstellungen\n",
    "ticker      = \"MSFT\"\n",
    "window_size = 10\n",
    "n_splits    = 3\n",
    "csv_path    = f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat_with_RSI.csv\"\n",
    "\n",
    "# 1) Daten einlesen und Return + GARCH_vol berechnen\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# GARCH einmalig auf allen Daten fitten\n",
    "scaled_all = df[\"Return\"] * 10\n",
    "garch_all  = arch_model(scaled_all, mean=\"Zero\", vol=\"GARCH\", p=1, q=1,\n",
    "                        dist=\"normal\", rescale=False).fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = garch_all.conditional_volatility / 10\n",
    "\n",
    "# 2) Statistische Features skalieren (RSI + GARCH_vol)\n",
    "static_cols = [\"GARCH_vol\", \"RSI_14\"]\n",
    "scaler = StandardScaler()\n",
    "df[static_cols] = scaler.fit_transform(df[static_cols])\n",
    "\n",
    "# 3) X/y Erzeuger\n",
    "def make_xy(df):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(df)):\n",
    "        seq  = df[\"Return\"].iloc[i-window_size:i].tolist()\n",
    "        stat = df[static_cols].iloc[i].tolist()\n",
    "        X.append(seq + stat)\n",
    "        y.append(df[\"Return\"].iat[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4) Hyperparameter‑Grid und CV-Setup\n",
    "param_grid = {\"units\":[50], \"dropout\":[0.2], \"lr\":[1e-3, 5e-4], \"batch_size\":[16]}\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "best_rmse, best_cfg = np.inf, None\n",
    "\n",
    "# 5) Grid‑Search über Log‑Return‑RMSE\n",
    "for units in param_grid[\"units\"]:\n",
    "    for drop in param_grid[\"dropout\"]:\n",
    "        for lr in param_grid[\"lr\"]:\n",
    "            for bs in param_grid[\"batch_size\"]:\n",
    "                cv_rmses = []\n",
    "                for tr_idx, te_idx in tscv.split(df):\n",
    "                    train_df = df.iloc[tr_idx]\n",
    "                    test_df  = df.iloc[te_idx]\n",
    "                    X_tr, y_tr = make_xy(train_df)\n",
    "                    X_ts, y_ts = make_xy(test_df)\n",
    "                    cut = int(len(X_tr)*0.9)\n",
    "                    X_train, X_val = X_tr[:cut], X_tr[cut:]\n",
    "                    y_train, y_val = y_tr[:cut], y_tr[cut:]\n",
    "                    X_train = X_train.reshape((-1, X_train.shape[1],1))\n",
    "                    X_val   = X_val.reshape((-1, X_val.shape[1],1))\n",
    "                    X_test  = X_ts .reshape((-1, X_ts .shape[1],1))\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(X_train.shape[1],1)),\n",
    "                        LSTM(units, return_sequences=True),\n",
    "                        Dropout(drop),\n",
    "                        LSTM(units),\n",
    "                        Dropout(drop),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                    model.compile(optimizer=opt, loss=\"mse\")\n",
    "                    es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "                    model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                              epochs=20, batch_size=bs, callbacks=[es], verbose=0)\n",
    "                    y_pred = model.predict(X_test).flatten()\n",
    "                    cv_rmses.append(math.sqrt(mean_squared_error(y_ts, y_pred)))\n",
    "                avg_rmse = np.mean(cv_rmses)\n",
    "                if avg_rmse < best_rmse:\n",
    "                    best_rmse, best_cfg = avg_rmse, {\"units\":units,\"dropout\":drop,\"lr\":lr,\"batch_size\":bs}\n",
    "print(f\"\\nBest CV‑RMSE (Log‑Renditen): {best_rmse:.4f}\")\n",
    "print(\"Best Config:\", best_cfg)\n",
    "\n",
    "# 5) Finales Modelltraining auf dem gesamten Datensatz\n",
    "X_all, y_all = make_xy(df)\n",
    "X_all = X_all.reshape((-1, X_all.shape[1], 1))\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_all.shape[1], 1)),\n",
    "    LSTM(best_cfg[\"units\"], return_sequences=True),\n",
    "    Dropout(best_cfg[\"dropout\"]),\n",
    "    LSTM(best_cfg[\"units\"]),\n",
    "    Dropout(best_cfg[\"dropout\"]),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(best_cfg[\"lr\"]), loss=\"mse\")\n",
    "model.fit(X_all, y_all, epochs=20, batch_size=best_cfg[\"batch_size\"], verbose=1)\n",
    "\n",
    "# Modellperformance auf Trainingdaten berechnen\n",
    "y_pred_all = model.predict(X_all).flatten()\n",
    "\n",
    "# RMSE auf Renditeebene (Train)\n",
    "rmse_ret_all = math.sqrt(mean_squared_error(y_all, y_pred_all))\n",
    "print(f\"\\n📈 RMSE (Renditeebene) auf Trainingsdaten: {rmse_ret_all:.4f}\")\n",
    "\n",
    "# RMSE auf Preisebene rekonstruieren\n",
    "# Schritt 1: Startpreis\n",
    "start_prices = df[\"Close\"].iloc[window_size - 1 : -1].values\n",
    "true_prices = df[\"Close\"].iloc[window_size:].values\n",
    "pred_prices = start_prices * np.exp(y_pred_all)\n",
    "\n",
    "# Schritt 2: RMSE auf Preisbasis\n",
    "rmse_prc_all = math.sqrt(mean_squared_error(true_prices, pred_prices))\n",
    "print(f\"💰 RMSE (Preisebene) auf Trainingsdaten:  {rmse_prc_all:.4f}\")\n",
    "\n",
    "# Modell speichern\n",
    "model.save(f\"../../05_Modelle/garch_lstm_{ticker.lower()}_final_model.keras\")\n",
    "print(\"✅ Finales Modell gespeichert\")\n",
    "\n",
    "\n",
    "# 6) Endgültiges Training & Metriken pro Fold ausgeben\n",
    "metrics = {\"rmse_ret\":[],\"mae_ret\":[],\"mape_ret\":[],\"r2_ret\":[],\"hit_ret\":[],\"sharpe_ret\":[],\n",
    "           \"rmse_prc\":[],\"mae_prc\":[],\"mape_prc\":[],\"r2_prc\":[],\"hit_prc\":[],\"sharpe_prc\":[]}\n",
    "fold_results = []\n",
    "\n",
    "for fold, (tr_idx, te_idx) in enumerate(tscv.split(df), 1):\n",
    "    train_df = df.iloc[tr_idx]\n",
    "    test_df  = df.iloc[te_idx]\n",
    "    X_tr, y_tr = make_xy(train_df)\n",
    "    X_ts, y_ts = make_xy(test_df)\n",
    "    X_tr = X_tr.reshape((-1, X_tr.shape[1],1))\n",
    "    X_ts = X_ts.reshape((-1, X_ts.shape[1],1))\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_tr.shape[1],1)),\n",
    "        LSTM(best_cfg[\"units\"], return_sequences=True),\n",
    "        Dropout(best_cfg[\"dropout\"]),\n",
    "        LSTM(best_cfg[\"units\"]),\n",
    "        Dropout(best_cfg[\"dropout\"]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(best_cfg[\"lr\"]), loss=\"mse\")\n",
    "    es = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "    model.fit(X_tr, y_tr, epochs=20, batch_size=best_cfg[\"batch_size\"], callbacks=[es], verbose=0)\n",
    "    y_pred = model.predict(X_ts).flatten()\n",
    "\n",
    "    # Returns-Metriken\n",
    "    rm_ret = math.sqrt(mean_squared_error(y_ts, y_pred))\n",
    "    mae_ret = mean_absolute_error(y_ts, y_pred)\n",
    "    denom_r = np.where(y_ts==0, np.nan, y_ts)\n",
    "    mape_ret = np.nanmean(np.abs((y_ts - y_pred)/denom_r))*100\n",
    "    r2_ret = r2_score(y_ts, y_pred)\n",
    "    dir_true = np.sign(np.diff(y_ts))\n",
    "    dir_pred = np.sign(np.diff(y_pred))\n",
    "    hit_ret = (dir_true==dir_pred).mean()*100\n",
    "    ret_ret = np.diff(y_pred)/y_pred[:-1]\n",
    "    sharpe_ret = ret_ret.mean()/(ret_ret.std() if ret_ret.std()!=0 else np.nan)\n",
    "    metrics[\"rmse_ret\"].append(rm_ret)\n",
    "    metrics[\"mae_ret\"].append(mae_ret)\n",
    "    metrics[\"mape_ret\"].append(mape_ret)\n",
    "    metrics[\"r2_ret\"].append(r2_ret)\n",
    "    metrics[\"hit_ret\"].append(hit_ret)\n",
    "    metrics[\"sharpe_ret\"].append(sharpe_ret)\n",
    "\n",
    "    # Preis-Metriken\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        p0 = test_df[\"Close\"].iloc[i+window_size-1]\n",
    "        p = p0 * np.exp(r)\n",
    "        preds.append(p)\n",
    "        actuals.append(test_df[\"Close\"].iloc[i+window_size])\n",
    "    preds = np.array(preds); actuals = np.array(actuals)\n",
    "    rm_prc = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    mae_prc = mean_absolute_error(actuals, preds)\n",
    "    denom_p = np.where(actuals==0, np.nan, actuals)\n",
    "    mape_prc = np.nanmean(np.abs((actuals - preds)/denom_p))*100\n",
    "    r2_prc = r2_score(actuals, preds)\n",
    "    dir_t = np.sign(np.diff(actuals))\n",
    "    dir_p = np.sign(np.diff(preds))\n",
    "    hit_prc = (dir_t==dir_p).mean()*100\n",
    "    ret_pr = np.diff(preds)/preds[:-1]\n",
    "    sharpe_prc = ret_pr.mean()/(ret_pr.std() if ret_pr.std()!=0 else np.nan)\n",
    "    metrics[\"rmse_prc\"].append(rm_prc)\n",
    "    metrics[\"mae_prc\"].append(mae_prc)\n",
    "    metrics[\"mape_prc\"].append(mape_prc)\n",
    "    metrics[\"r2_prc\"].append(r2_prc)\n",
    "    metrics[\"hit_prc\"].append(hit_prc)\n",
    "    metrics[\"sharpe_prc\"].append(sharpe_prc)\n",
    "    print(f\"Fold {fold}:\\n\"\n",
    "          f\"  Returns → RMSE={rm_ret:.4f}, MAE={mae_ret:.4f}, MAPE={mape_ret:.4f}%, R2={r2_ret:.4f}, Hit-Rate={hit_ret:.4f}%, Sharpe={sharpe_ret:.4f}\\n\"\n",
    "          f\"  Prices  → RMSE={rm_prc:.4f}, MAE={mae_prc:.4f}, MAPE={mape_prc:.4f}%, R2={r2_prc:.4f}, Hit-Rate={hit_prc:.4f}%, Sharpe={sharpe_prc:.4f}\")\n",
    "    fold_results.append({\"idx\":test_df.index[window_size:], \"y_test\":y_ts, \"y_pred\":y_pred,\n",
    "                         \"actuals\":actuals, \"preds\":preds})\n",
    "\n",
    "# 7) Durchschnittliche Metriken über alle Folds\n",
    "# berechne Mittelwerte\n",
    "avg_rmse_ret   = np.nanmean(metrics[\"rmse_ret\"])\n",
    "avg_mae_ret    = np.nanmean(metrics[\"mae_ret\"])\n",
    "avg_mape_ret   = np.nanmean(metrics[\"mape_ret\"])\n",
    "avg_r2_ret     = np.nanmean(metrics[\"r2_ret\"])\n",
    "avg_hit_ret    = np.nanmean(metrics[\"hit_ret\"])\n",
    "avg_sharpe_ret = np.nanmean(metrics[\"sharpe_ret\"])\n",
    "\n",
    "avg_rmse_prc   = np.nanmean(metrics[\"rmse_prc\"])\n",
    "avg_mae_prc    = np.nanmean(metrics[\"mae_prc\"])\n",
    "avg_mape_prc   = np.nanmean(metrics[\"mape_prc\"])\n",
    "avg_r2_prc     = np.nanmean(metrics[\"r2_prc\"])\n",
    "avg_hit_prc    = np.nanmean(metrics[\"hit_prc\"])\n",
    "avg_sharpe_prc = np.nanmean(metrics[\"sharpe_prc\"])\n",
    "\n",
    "# Ausgabe\n",
    "print(\"\\n=== Durchschnittliche Metriken: Log-Renditen ===\")\n",
    "print(f\"RMSE      = {avg_rmse_ret:.4f}\")\n",
    "print(f\"MAE       = {avg_mae_ret:.4f}\")\n",
    "print(f\"MAPE      = {avg_mape_ret:,.4f}%\")\n",
    "print(f\"R²        = {avg_r2_ret:.4f}\")\n",
    "print(f\"HitRate   = {avg_hit_ret:.4f}%\")\n",
    "print(f\"Sharpe    = {avg_sharpe_ret:.4f}\")\n",
    "\n",
    "print(\"\\n=== Durchschnittliche Metriken: Preise ===\")\n",
    "print(f\"RMSE      = {avg_rmse_prc:.4f}\")\n",
    "print(f\"MAE       = {avg_mae_prc:.4f}\")\n",
    "print(f\"MAPE      = {avg_mape_prc:.4f}%\")\n",
    "print(f\"R²        = {avg_r2_prc:.4f}\")\n",
    "print(f\"HitRate   = {avg_hit_prc:.4f}%\")\n",
    "print(f\"Sharpe    = {avg_sharpe_prc:.4f}\")\n",
    "\n",
    "# 8) Plots: Log-Renditen & Preise über alle Folds\n",
    "plt.figure(figsize=(12,5))\n",
    "for i,fr in enumerate(fold_results,1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_test\"], label=f\"Real Ret Fold {i}\", alpha=0.8)\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_pred\"], \"--\", label=f\"Pred Ret Fold {i}\", alpha=0.8)\n",
    "plt.title(f\"{ticker} – Log‑Renditen alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log‑Rendite\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "for i,fr in enumerate(fold_results,1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"actuals\"], label=f\"Real Prc Fold {i}\", alpha=0.8)\n",
    "    plt.plot(fr[\"idx\"], fr[\"preds\"],   \"--\", label=f\"Pred Prc Fold {i}\", alpha=0.8)\n",
    "plt.title(f\"{ticker} – Aktienkurse alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ],
   "id": "4f721b94bdba721e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_LSTM_RSI_RMSE_Test.py",
   "id": "fcc5dd9d9eab1015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from arch import arch_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 1) Daten laden und auf die letzten 30 Tage beschränken\n",
    "ticker = \"MSFT\"\n",
    "df = pd.read_csv(f\"../../03_Daten/processed_data/historical_stock_data_daily_{ticker}_last60d_flat_with_RSI.csv\", parse_dates=[\"Date\"])\n",
    "df = df.sort_values(\"Date\").set_index(\"Date\")\n",
    "df = df.last(\"30D\").copy()\n",
    "\n",
    "# 2) Return und GARCH-Volatilität berechnen\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(subset=[\"Return\", \"RSI_14\"], inplace=True)\n",
    "\n",
    "# GARCH auf Return anwenden\n",
    "scaled_ret = df[\"Return\"] * 10\n",
    "garch = arch_model(scaled_ret, mean=\"Zero\", vol=\"GARCH\", p=1, q=1, dist=\"normal\", rescale=False).fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = garch.conditional_volatility / 10\n",
    "\n",
    "# 3) Feature-Skalierung (wie im Training: StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "df[[\"GARCH_vol\", \"RSI_14\"]] = scaler.fit_transform(df[[\"GARCH_vol\", \"RSI_14\"]])\n",
    "\n",
    "# 4) Sequenzen erzeugen (wie im Training)\n",
    "window_size = 10\n",
    "def make_xy(df):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(df)):\n",
    "        seq  = df[\"Return\"].iloc[i - window_size:i].tolist()\n",
    "        stat = df[[\"GARCH_vol\", \"RSI_14\"]].iloc[i].tolist()\n",
    "        X.append(seq + stat)\n",
    "        y.append(df[\"Return\"].iloc[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_test, y_test = make_xy(df)\n",
    "X_test = X_test.reshape((-1, X_test.shape[1], 1))\n",
    "\n",
    "# 5) Modell laden und Vorhersage durchführen\n",
    "model = load_model(f\"../../05_Modelle/garch_lstm_{ticker.lower()}_final_model.keras\")\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# 6) RMSE berechnen\n",
    "rmse_day  = math.sqrt(mean_squared_error(y_test[-1:], y_pred[-1:]))\n",
    "rmse_week = math.sqrt(mean_squared_error(y_test[-7:], y_pred[-7:]))\n",
    "rmse_full = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 7) Ergebnis anzeigen\n",
    "print(\"\\nRMSE auf den letzten 30 Tagen:\")\n",
    "print(f\"Letzter Tag    : {rmse_day:.4f}\")\n",
    "print(f\"Letzte Woche   : {rmse_week:.4f}\")\n",
    "print(f\"Letzte 30 Tage : {rmse_full:.4f}\")\n",
    "\n",
    "# 8) RMSE auf tatsächliche Preise berechnen\n",
    "# Rekonstruiere Preise aus vorhergesagten Log-Renditen\n",
    "pred_prices = []\n",
    "real_prices = []\n",
    "\n",
    "# Ausgangspunkt: Preis an Position window_size - 1\n",
    "start_idx = window_size - 1\n",
    "for i in range(len(y_pred)):\n",
    "    p0 = df[\"Close\"].iloc[start_idx + i]  # letzter bekannter Preis\n",
    "    pred_price = p0 * np.exp(y_pred[i])\n",
    "    real_price = df[\"Close\"].iloc[start_idx + i + 1]\n",
    "    pred_prices.append(pred_price)\n",
    "    real_prices.append(real_price)\n",
    "\n",
    "pred_prices = np.array(pred_prices)\n",
    "real_prices = np.array(real_prices)\n",
    "\n",
    "# Preis-RMSE berechnen\n",
    "rmse_price_day  = math.sqrt(mean_squared_error(real_prices[-1:], pred_prices[-1:]))\n",
    "rmse_price_week = math.sqrt(mean_squared_error(real_prices[-7:], pred_prices[-7:]))\n",
    "rmse_price_full = math.sqrt(mean_squared_error(real_prices, pred_prices))\n",
    "\n",
    "# Ausgabe\n",
    "print(\"\\nRMSE auf den tatsächlichen Aktienkursen:\")\n",
    "print(f\"Letzter Tag    : {rmse_price_day:.4f}\")\n",
    "print(f\"Letzte Woche   : {rmse_price_week:.4f}\")\n",
    "print(f\"Letzte 30 Tage : {rmse_price_full:.4f}\")\n"
   ],
   "id": "f6683431a7e4a4d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_XGBoost2.py",
   "id": "68be36dddd6f97c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import math\n",
    "\n",
    "# 1) Daten einlesen und Renditen berechnen\n",
    "ticker = \"NVDA\"\n",
    "df = pd.read_csv(\n",
    "    f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat.csv\",\n",
    "    parse_dates=[\"Date\"], index_col=\"Date\"\n",
    ")\n",
    "df.sort_index(inplace=True)\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 2) TimeSeriesSplit konfigurieren\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "window_size = 10\n",
    "\n",
    "# 3) Helper: Features und Target aus Returns + GARCH-Vol erstellen\n",
    "def create_features_and_target(df, window_size=10):\n",
    "    X, y = [], []\n",
    "    rets = df[\"Return\"].values\n",
    "    vols = df[\"GARCH_vol\"].values\n",
    "    for i in range(window_size, len(df)):\n",
    "        feats = np.concatenate([rets[i-window_size:i], [vols[i]]])\n",
    "        X.append(feats)\n",
    "        y.append(rets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4) Speicher für Metriken und Plot-Daten\n",
    "metrics_returns = []\n",
    "metrics_prices  = []\n",
    "\n",
    "dates_ret_all = []\n",
    "y_test_all    = []\n",
    "y_pred_all    = []\n",
    "\n",
    "dates_pr_all  = []\n",
    "actuals_all   = []\n",
    "preds_all     = []\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in tscv.split(df):\n",
    "    train_df = df.iloc[train_idx].copy()\n",
    "    test_df  = df.iloc[test_idx].copy()\n",
    "\n",
    "    # a) GARCH auf Trainingsdaten fitten\n",
    "    scaled = train_df[\"Return\"] * 10\n",
    "    g = arch_model(scaled, mean=\"Zero\", vol=\"GARCH\", p=1, q=1,\n",
    "                   dist=\"normal\", rescale=False)\n",
    "    res = g.fit(disp=\"off\")\n",
    "    train_df[\"GARCH_vol\"] = res.conditional_volatility / 10\n",
    "\n",
    "    # b) Forecast der Volatilität für Testperiode\n",
    "    fc = res.forecast(start=train_df.index[-1],\n",
    "                      horizon=len(test_df),\n",
    "                      reindex=False)\n",
    "    test_df[\"GARCH_vol\"] = np.sqrt(fc.variance.values[-1, :]) / 10\n",
    "\n",
    "    # c) Features & Targets für XGB\n",
    "    X_train, y_train = create_features_and_target(train_df, window_size)\n",
    "    X_test,  y_test  = create_features_and_target(test_df,  window_size)\n",
    "\n",
    "    # d) XGBoost trainieren und vorhersagen\n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3,\n",
    "                         learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Datums-Indizes\n",
    "    dates = test_df.index[window_size:]\n",
    "    # für Renditen-Plot\n",
    "    dates_ret_all.append(dates)\n",
    "    y_test_all.append(y_test)\n",
    "    y_pred_all.append(y_pred)\n",
    "\n",
    "    # e) Metriken für Log-Renditen\n",
    "    rmse_ret    = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae_ret     = mean_absolute_error(y_test, y_pred)\n",
    "    mape_ret    = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "    r2_ret      = r2_score(y_test, y_pred)\n",
    "    hitrate_ret = np.mean(np.sign(y_test) == np.sign(y_pred)) * 100\n",
    "    sharpe_ret  = np.mean(y_pred) / (np.std(y_pred) + 1e-8)\n",
    "\n",
    "    metrics_returns.append([\n",
    "        rmse_ret, mae_ret, mape_ret,\n",
    "        r2_ret, hitrate_ret, sharpe_ret\n",
    "    ])\n",
    "\n",
    "    # f) Rückrechnung in Close-Preise\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        idx = i + window_size\n",
    "        prev_p = test_df[\"Close\"].iloc[idx-1]\n",
    "        p_pred = prev_p * np.exp(r)\n",
    "        preds.append(p_pred)\n",
    "        actuals.append(test_df[\"Close\"].iloc[idx])\n",
    "    preds   = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    # für Preis-Plot\n",
    "    dates_pr_all.append(dates)\n",
    "    actuals_all.append(actuals)\n",
    "    preds_all.append(preds)\n",
    "\n",
    "    # g) Metriken auf Preisbasis\n",
    "    rmse_price   = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    mae_price    = mean_absolute_error(actuals, preds)\n",
    "    mape_price   = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
    "    r2_price     = r2_score(actuals, preds)\n",
    "    hitrate_price= np.mean(np.sign(np.diff(actuals)) ==\n",
    "                           np.sign(np.diff(preds))) * 100\n",
    "    sharpe_price = np.mean(np.diff(preds)) / (np.std(np.diff(preds)) + 1e-8)\n",
    "\n",
    "    metrics_prices.append([\n",
    "        rmse_price, mae_price, mape_price,\n",
    "        r2_price, hitrate_price, sharpe_price\n",
    "    ])\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE_Returns={rmse_ret:.4f}, RMSE_Prices={rmse_price:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# 5) Durchschnittliche Metriken ausgeben\n",
    "def print_avg(name, arr):\n",
    "    m = np.array(arr)\n",
    "    print(f\"\\n=== Ø Metriken: {name} ===\")\n",
    "    print(f\"RMSE   = {m[:,0].mean():.4f}\")\n",
    "    print(f\"MAE    = {m[:,1].mean():.4f}\")\n",
    "    print(f\"MAPE   = {m[:,2].mean():.2f}%\")\n",
    "    print(f\"R²     = {m[:,3].mean():.4f}\")\n",
    "    print(f\"HitRate= {m[:,4].mean():.2f}%\")\n",
    "    print(f\"Sharpe = {m[:,5].mean():.4f}\")\n",
    "\n",
    "print_avg(\"Log-Renditen\", metrics_returns)\n",
    "print_avg(\"Close-Preise\", metrics_prices)\n",
    "\n",
    "# 6) Plot aller 3 Folds: Log-Renditen\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_ret_all[i],    y_test_all[i],\n",
    "             label=f\"Real Ret Fold {i+1}\")\n",
    "    plt.plot(dates_ret_all[i],    y_pred_all[i],\n",
    "             linestyle=\"--\", label=f\"Pred Ret Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Log-Renditen alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log-Return\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 7) Plot aller 3 Folds: Close-Preise\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_pr_all[i], actuals_all[i],\n",
    "             label=f\"Real Prc Fold {i+1}\")\n",
    "    plt.plot(dates_pr_all[i], preds_all[i],\n",
    "             linestyle=\"--\", label=f\"Pred Prc Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Close-Preise alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ],
   "id": "8c0c2fb2bcf0e6c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_XGBoost_RSI.py",
   "id": "49cac0ab68bbf645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Einstellungen\n",
    "ticker      = \"MSFT\"\n",
    "window_size = 10\n",
    "n_splits    = 3\n",
    "csv_path    = f\"../../03_Daten/processed_data/historical_stock_data_weekly_{ticker}_flat_with_RSI.csv\"\n",
    "\n",
    "# 1) Daten einlesen und Log-Renditen berechnen\n",
    "df = (\n",
    "    pd.read_csv(csv_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "      .sort_index()\n",
    ")\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 2) GARCH einmalig auf gesamten Datensatz fitten und Volatilität speichern\n",
    "res_all = arch_model(df[\"Return\"] * 10,\n",
    "                     mean=\"Zero\", vol=\"GARCH\", p=1, q=1,\n",
    "                     dist=\"normal\", rescale=False\n",
    "                    ).fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = res_all.conditional_volatility / 10\n",
    "\n",
    "# 3) Helper zum Erzeugen der Features/Targets\n",
    "def make_xy(df, window_size=10):\n",
    "    X, y = [], []\n",
    "    rets = df[\"Return\"].values\n",
    "    vols = df[\"GARCH_vol\"].values\n",
    "    rsis = df[\"RSI_14\"].values\n",
    "    for i in range(window_size, len(df)):\n",
    "        seq   = rets[i-window_size:i]\n",
    "        feats = np.concatenate([seq, [vols[i], rsis[i]]])\n",
    "        X.append(feats)\n",
    "        y.append(rets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4) Hyperparam-Suche\n",
    "param_dist = {\n",
    "    \"n_estimators\":    [50, 100, 200],\n",
    "    \"max_depth\":       [3, 5, 7],\n",
    "    \"learning_rate\":   [0.01, 0.05, 0.1],\n",
    "    \"subsample\":       [0.8, 1.0],\n",
    "    \"colsample_bytree\":[0.8, 1.0],\n",
    "    \"gamma\":           [0, 0.1, 0.5]\n",
    "}\n",
    "xgb_base = XGBRegressor(random_state=42, tree_method=\"hist\")\n",
    "tscv_search = TimeSeriesSplit(n_splits=n_splits)\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_base, param_dist,\n",
    "    n_iter=20,\n",
    "    cv=tscv_search,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "X_full, y_full = make_xy(df, window_size)\n",
    "search.fit(X_full, y_full)\n",
    "best_params = search.best_params_\n",
    "print(\">>> Best XGBoost Params:\", best_params)\n",
    "\n",
    "# 5) CV: Train/Test, Metriken, und Sammeln für Plots\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Listen für durchschnittliche Metriken\n",
    "metrics_returns = []\n",
    "metrics_prices  = []\n",
    "\n",
    "# Listen für Plot-Daten aller Folds\n",
    "dates_ret_all    = []\n",
    "ret_true_all     = []\n",
    "ret_pred_all     = []\n",
    "dates_price_all  = []\n",
    "price_true_all   = []\n",
    "price_pred_all   = []\n",
    "\n",
    "fold = 1\n",
    "for tr_idx, te_idx in tscv.split(df):\n",
    "    train_df = df.iloc[tr_idx].copy()\n",
    "    test_df  = df.iloc[te_idx].copy()\n",
    "\n",
    "    # GARCH-Forecast für Test-Set\n",
    "    fc = res_all.forecast(\n",
    "        start=train_df.index[-1],\n",
    "        horizon=len(test_df),\n",
    "        reindex=False\n",
    "    )\n",
    "    test_df[\"GARCH_vol\"] = np.sqrt(fc.variance.values[-1, :]) / 10\n",
    "\n",
    "    # Features/Targets\n",
    "    X_train, y_train = make_xy(train_df, window_size)\n",
    "    X_test,  y_test  = make_xy(test_df,  window_size)\n",
    "\n",
    "    # Finales Modell trainieren\n",
    "    model = XGBRegressor(**best_params, random_state=42, tree_method=\"hist\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Vorhersage Log-Returns\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Datums-Indizes\n",
    "    dates = test_df.index[window_size:]\n",
    "    dates_ret_all.append(dates)\n",
    "    ret_true_all.append(y_test)\n",
    "    ret_pred_all.append(y_pred)\n",
    "\n",
    "    # Metriken Log-Returns mit Filter für Null-Returns\n",
    "    rmse_ret    = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae_ret     = mean_absolute_error(y_test, y_pred)\n",
    "    mask        = (y_test != 0)\n",
    "    mape_ret    = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100\n",
    "    r2_ret      = r2_score(y_test, y_pred)\n",
    "    hitrate_ret = np.mean(np.sign(y_test) == np.sign(y_pred)) * 100\n",
    "    sharpe_ret  = np.mean(y_pred) / (np.std(y_pred) + 1e-8)\n",
    "    metrics_returns.append([rmse_ret, mae_ret, mape_ret,\n",
    "                             r2_ret, hitrate_ret, sharpe_ret])\n",
    "\n",
    "    # Rückrechnung auf Close-Preise\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        p0 = test_df[\"Close\"].iat[i + window_size - 1]\n",
    "        p_pred = p0 * np.exp(r)\n",
    "        preds.append(p_pred)\n",
    "        actuals.append(test_df[\"Close\"].iat[i + window_size])\n",
    "    preds   = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    dates_price_all.append(dates)\n",
    "    price_true_all.append(actuals)\n",
    "    price_pred_all.append(preds)\n",
    "\n",
    "    # Metriken Close-Preise\n",
    "    rmse_p    = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    mae_p     = mean_absolute_error(actuals, preds)\n",
    "    mape_p    = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
    "    r2_p      = r2_score(actuals, preds)\n",
    "    hitrate_p = np.mean(np.sign(np.diff(actuals)) == np.sign(np.diff(preds))) * 100\n",
    "    sharpe_p  = np.mean(np.diff(preds)) / (np.std(np.diff(preds)) + 1e-8)\n",
    "    metrics_prices.append([rmse_p, mae_p, mape_p,\n",
    "                           r2_p, hitrate_p, sharpe_p])\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE_Returns={rmse_ret:.4f}, RMSE_Prices={rmse_p:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# 6) Durchschnittliche Metriken ausgeben\n",
    "def print_avg(name, arr):\n",
    "    m = np.array(arr)\n",
    "    print(f\"\\n=== Ø Metriken: {name} ===\")\n",
    "    print(f\"RMSE    = {m[:,0].mean():.4f}\")\n",
    "    print(f\"MAE     = {m[:,1].mean():.4f}\")\n",
    "    print(f\"MAPE    = {m[:,2].mean():.2f}%\")\n",
    "    print(f\"R²      = {m[:,3].mean():.4f}\")\n",
    "    print(f\"HitRate = {m[:,4].mean():.2f}%\")\n",
    "    print(f\"Sharpe  = {m[:,5].mean():.4f}\")\n",
    "\n",
    "print_avg(\"Log-Renditen\", metrics_returns)\n",
    "print_avg(\"Close-Preise\", metrics_prices)\n",
    "\n",
    "# 7) Plot: Log-Renditen über alle 3 Folds\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_ret_all[i], ret_true_all[i],  label=f\"Real Ret Fold {i+1}\")\n",
    "    plt.plot(dates_ret_all[i], ret_pred_all[i], linestyle=\"--\", label=f\"Pred Ret Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Log-Renditen alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log-Return\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 8) Plot: Close-Preise über alle 3 Folds\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(n_splits):\n",
    "    plt.plot(dates_price_all[i], price_true_all[i],  label=f\"Real Prc Fold {i+1}\")\n",
    "    plt.plot(dates_price_all[i], price_pred_all[i], linestyle=\"--\", label=f\"Pred Prc Fold {i+1}\")\n",
    "plt.title(f\"{ticker} – Close-Preise alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# 9) Finales Modell auf allen historischen Daten trainieren\n",
    "model_final = XGBRegressor(**best_params, random_state=42, tree_method=\"hist\")\n",
    "model_final.fit(X_full, y_full)\n",
    "\n",
    "# 10) Vorhersage auf Trainingsdaten\n",
    "y_full_pred = model_final.predict(X_full)\n",
    "\n",
    "# RMSE Log-Rendite (Training)\n",
    "rmse_ret_full = math.sqrt(mean_squared_error(y_full, y_full_pred))\n",
    "print(f\"\\n📈 RMSE (Renditeebene) auf Trainingsdaten: {rmse_ret_full:.4f}\")\n",
    "\n",
    "# Preis-Rückrechnung\n",
    "start_prices = df[\"Close\"].iloc[window_size - 1 : -1].values\n",
    "true_prices  = df[\"Close\"].iloc[window_size:].values\n",
    "pred_prices  = start_prices * np.exp(y_full_pred)\n",
    "\n",
    "# RMSE Preis (Training)\n",
    "rmse_prc_full = math.sqrt(mean_squared_error(true_prices, pred_prices))\n",
    "print(f\"💰 RMSE (Preisebene) auf Trainingsdaten:  {rmse_prc_full:.4f}\")\n",
    "\n",
    "# 11) Modell speichern\n",
    "model_path = f\"../../05_Modelle/garch_xgboost_{ticker.lower()}_final_model.joblib\"\n",
    "dump(model_final, model_path)\n",
    "print(f\"✅ Modell gespeichert: {model_path}\")\n"
   ],
   "id": "1504d75f14aadac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_XGBoost_RSI_RMSE_Test.py",
   "id": "5c9949b532cd015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from joblib import load\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Parameter\n",
    "ticker = \"MSFT\"\n",
    "window_size = 10\n",
    "model_path = f\"../../05_Modelle/garch_xgboost_{ticker.lower()}_final_model.joblib\"\n",
    "csv_path = f\"../../03_Daten/processed_data/historical_stock_data_daily_{ticker}_last60d_flat_with_RSI.csv\"\n",
    "\n",
    "# Modell laden\n",
    "model = load(model_path)\n",
    "print(f\"✅ Modell geladen: {model_path}\")\n",
    "\n",
    "# Daten laden und vorbereiten\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"Date\"])\n",
    "df = df.sort_values(\"Date\").set_index(\"Date\")\n",
    "df = df.loc[df.index >= df.index.max() - pd.Timedelta(days=30)].copy()\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(subset=[\"Return\", \"RSI_14\"], inplace=True)\n",
    "\n",
    "# GARCH-Volatilität berechnen\n",
    "ret_scaled = df[\"Return\"] * 10\n",
    "garch = arch_model(ret_scaled, mean=\"Zero\", vol=\"GARCH\", p=1, q=1, dist=\"normal\", rescale=False).fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = garch.conditional_volatility / 10\n",
    "\n",
    "# Feature-Vektoren erstellen\n",
    "def make_xy_outsample(df, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(df)):\n",
    "        seq = df[\"Return\"].iloc[i - window_size:i].values\n",
    "        vol = df[\"GARCH_vol\"].iloc[i]\n",
    "        rsi = df[\"RSI_14\"].iloc[i]\n",
    "        X.append(np.concatenate([seq, [vol, rsi]]))\n",
    "        y.append(df[\"Return\"].iloc[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_30, y_30 = make_xy_outsample(df, window_size)\n",
    "y_pred = model.predict(X_30)\n",
    "\n",
    "# RMSE Log-Rendite\n",
    "rmse_ret_day  = math.sqrt(mean_squared_error(y_30[-1:], y_pred[-1:]))\n",
    "rmse_ret_week = math.sqrt(mean_squared_error(y_30[-7:], y_pred[-7:]))\n",
    "rmse_ret_full = math.sqrt(mean_squared_error(y_30, y_pred))\n",
    "print(\"\\n📈 RMSE auf Log-Renditen (Out-of-sample)\")\n",
    "print(f\"Letzter Tag    : {rmse_ret_day:.4f}\")\n",
    "print(f\"Letzte Woche   : {rmse_ret_week:.4f}\")\n",
    "print(f\"Letzte 30 Tage : {rmse_ret_full:.4f}\")\n",
    "\n",
    "# RMSE Preisprognose\n",
    "pred_prices, real_prices = [], []\n",
    "start_idx = window_size - 1\n",
    "for i in range(len(y_pred)):\n",
    "    p0 = df[\"Close\"].iloc[start_idx + i]\n",
    "    p_pred = p0 * np.exp(y_pred[i])\n",
    "    p_real = df[\"Close\"].iloc[start_idx + i + 1]\n",
    "    pred_prices.append(p_pred)\n",
    "    real_prices.append(p_real)\n",
    "\n",
    "rmse_price_day  = math.sqrt(mean_squared_error(real_prices[-1:], pred_prices[-1:]))\n",
    "rmse_price_week = math.sqrt(mean_squared_error(real_prices[-7:], pred_prices[-7:]))\n",
    "rmse_price_full = math.sqrt(mean_squared_error(real_prices, pred_prices))\n",
    "print(\"\\n💰 RMSE auf Preisen (Out-of-sample)\")\n",
    "print(f\"Letzter Tag    : {rmse_price_day:.4f}\")\n",
    "print(f\"Letzte Woche   : {rmse_price_week:.4f}\")\n",
    "print(f\"Letzte 30 Tage : {rmse_price_full:.4f}\")\n"
   ],
   "id": "823e445febdf53a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Integration und Evaluation von Google Trends-Daten",
   "id": "d86b822db9199ca6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EDA.py",
   "id": "bdd192a628f5686c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "\n",
    "# Hochgeladene Dateien\n",
    "paths = {\n",
    "    \"NVDA\": \"../../03_Daten/processed_data/merged_weekly_NVDA_2015-2025.csv\",\n",
    "    \"GOOG\": \"../../03_Daten/processed_data/merged_weekly_GOOG_2015-2025.csv\",\n",
    "    \"MSFT\": \"../../03_Daten/processed_data/merged_weekly_MSFT_2015-2025.csv\"\n",
    "}\n",
    "\n",
    "# GTD-Spalten\n",
    "gtd_cols = {\n",
    "    \"NVDA\": [\"NVIDIA stock\", \"buy NVIDIA stock\", \"sell NVIDIA stock\"],\n",
    "    \"GOOG\": [\"Google stock\", \"buy Google stock\", \"sell Google stock\"],\n",
    "    \"MSFT\": [\"Microsoft stock\", \"buy Microsoft stock\", \"sell Microsoft stock\"]\n",
    "}\n",
    "\n",
    "for ticker, path in paths.items():\n",
    "    df = pd.read_csv(path, parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "    df[\"Return\"] = df[\"Close\"].pct_change()\n",
    "\n",
    "    print(f\"\\n=== {ticker}: Beschreibung ===\")\n",
    "    print(df.describe())\n",
    "\n",
    "    print(\"Fehlende Werte:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Plot: Schlusskurs\n",
    "    plt.plot(df[\"Close\"])\n",
    "    plt.title(f\"{ticker} – Wöchentlicher Schlusskurs\")\n",
    "    plt.xlabel(\"Datum\"); plt.ylabel(\"Close\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Plot: Histogramm Close\n",
    "    sns.histplot(df[\"Close\"].dropna(), kde=True)\n",
    "    plt.title(f\"{ticker} – Verteilung Schlusskurs\")\n",
    "    plt.xlabel(\"Close\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Plot: Boxplot Close\n",
    "    sns.boxplot(x=df[\"Close\"].dropna())\n",
    "    plt.title(f\"{ticker} – Boxplot Schlusskurs\")\n",
    "    plt.xlabel(\"Close\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Plot: Wöchentliche Renditen\n",
    "    plt.plot(df[\"Return\"])\n",
    "    plt.title(f\"{ticker} – Wöchentliche Rendite\")\n",
    "    plt.xlabel(\"Datum\"); plt.ylabel(\"Return\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Korrelationsmatrix zwischen Close und GTD\n",
    "    corr_cols = [\"Close\"] + gtd_cols[ticker]\n",
    "    corr_df = df[corr_cols].dropna()\n",
    "    corr_matrix = corr_df.corr()\n",
    "\n",
    "    print(\"Korrelationsmatrix:\")\n",
    "    print(corr_matrix)\n",
    "\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(f\"{ticker} – Korrelation: Schlusskurs und Google Trends\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ],
   "id": "b74a4b27e1477866",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### gtd_stock_corr.py",
   "id": "225d0924742640ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ticker-konfiguration\n",
    "ticker_files = {\n",
    "    \"NVDA\": {\n",
    "        \"file\": \"../../03_Daten/processed_data/merged_weekly_NVDA_2015-2025.csv\",\n",
    "        \"gtd\": [\"NVIDIA stock\", \"buy NVIDIA stock\", \"sell NVIDIA stock\"]\n",
    "    },\n",
    "    \"GOOG\": {\n",
    "        \"file\": \"../../03_Daten/processed_data/merged_weekly_GOOG_2015-2025.csv\",\n",
    "        \"gtd\": [\"Google stock\", \"buy Google stock\", \"sell Google stock\"]\n",
    "    },\n",
    "    \"MSFT\": {\n",
    "        \"file\": \"../../03_Daten/processed_data/merged_weekly_MSFT_2015-2025.csv\",\n",
    "        \"gtd\": [\"Microsoft stock\", \"buy Microsoft stock\", \"sell Microsoft stock\"]\n",
    "    },\n",
    "}\n",
    "\n",
    "for ticker, cfg in ticker_files.items():\n",
    "    df = pd.read_csv(cfg[\"file\"], parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "\n",
    "    # 4-Wochen-Rolling-Mean der GTD\n",
    "    df_roll = df[cfg[\"gtd\"]].rolling(window=4, min_periods=1).mean()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "    ax1.plot(df.index, df[\"Close\"], color=\"tab:blue\", linewidth=2, label=\"Close Price\")\n",
    "    ax1.set_xlabel(\"Datum\")\n",
    "    ax1.set_ylabel(\"Close Price\", color=\"tab:blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    styles = [\"--\", \"-.\", \":\"]\n",
    "    for col, ls in zip(cfg[\"gtd\"], styles):\n",
    "        ax2.plot(df_roll.index, df_roll[col],\n",
    "                 linestyle=ls, alpha=0.8, label=f\"{col} (4-Wochen MA)\")\n",
    "    ax2.set_ylabel(\"Google Trends (4-Wochen gleitend)\")\n",
    "\n",
    "    # Legenden an getrennten Ecken\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1, l1, loc=\"upper left\")\n",
    "    ax2.legend(h2, l2, loc=\"upper right\")\n",
    "\n",
    "    plt.title(f\"{ticker} – Kurs vs. Google Trends (4-Wochen-MA)\")\n",
    "    fig.tight_layout()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ],
   "id": "2857cf41a524b905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_LSTM_RSI_GTD.py",
   "id": "24e954db77bd7190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Einstellungen\n",
    "ticker      = \"NVDA\"\n",
    "window_size = 10\n",
    "n_splits    = 3\n",
    "csv_path    = f\"../../03_Daten/processed_data/merged_weekly_{ticker}_2015-2025_with_trends.csv\"\n",
    "\n",
    "# 1) Daten einlesen und Return + GARCH_vol berechnen\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# GARCH einmalig auf allen Daten fitten\n",
    "scaled_all = df[\"Return\"] * 10\n",
    "garch_all  = arch_model(scaled_all, mean=\"Zero\", vol=\"GARCH\", p=1, q=1,\n",
    "                        dist=\"normal\", rescale=False).fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = garch_all.conditional_volatility / 10\n",
    "\n",
    "# 2) Statistische Features skalieren & GTD-Spalten identifizieren\n",
    "static_cols = [\"GARCH_vol\", \"RSI_14\", \"Trend_Average\", \"Trend_Smoothed\"]\n",
    "print(\"Verwendete statische Features:\", static_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[static_cols] = scaler.fit_transform(df[static_cols])\n",
    "\n",
    "# 3) X/y Erzeuger\n",
    "def make_xy(df):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(df)):\n",
    "        seq  = df[\"Return\"].iloc[i-window_size:i].tolist()\n",
    "        stat = df[static_cols].iloc[i].tolist()\n",
    "        X.append(seq + stat)\n",
    "        y.append(df[\"Return\"].iat[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4) Hyperparameter‑Grid\n",
    "param_grid = {\n",
    "    \"units\":      [50],\n",
    "    \"dropout\":    [0.2],\n",
    "    \"lr\":         [1e-3, 5e-4],\n",
    "    \"batch_size\": [16]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "best_rmse, best_cfg = np.inf, None\n",
    "\n",
    "# 5) Grid‑Search über Log‑Return‑RMSE\n",
    "for units in param_grid[\"units\"]:\n",
    "    for drop in param_grid[\"dropout\"]:\n",
    "        for lr in param_grid[\"lr\"]:\n",
    "            for bs in param_grid[\"batch_size\"]:\n",
    "                cv_rmses = []\n",
    "                for tr_idx, te_idx in tscv.split(df):\n",
    "                    train_df = df.iloc[tr_idx]\n",
    "                    test_df  = df.iloc[te_idx]\n",
    "                    X_tr, y_tr = make_xy(train_df)\n",
    "                    X_ts, y_ts = make_xy(test_df)\n",
    "\n",
    "                    # Split Train/Val\n",
    "                    cut = int(len(X_tr) * 0.9)\n",
    "                    X_train, X_val = X_tr[:cut], X_tr[cut:]\n",
    "                    y_train, y_val = y_tr[:cut], y_tr[cut:]\n",
    "\n",
    "                    # reshape\n",
    "                    X_train = X_train.reshape((-1, X_train.shape[1], 1))\n",
    "                    X_val   = X_val.reshape((-1, X_val.shape[1], 1))\n",
    "                    X_test  = X_ts .reshape((-1, X_ts .shape[1], 1))\n",
    "\n",
    "                    # Modell\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(X_train.shape[1],1)),\n",
    "                        LSTM(units, return_sequences=True),\n",
    "                        Dropout(drop),\n",
    "                        LSTM(units),\n",
    "                        Dropout(drop),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                    model.compile(optimizer=opt, loss=\"mse\")\n",
    "                    es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "                    model.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val),\n",
    "                              epochs=20, batch_size=bs,\n",
    "                              callbacks=[es], verbose=0)\n",
    "\n",
    "                    # Evaluation\n",
    "                    y_pred = model.predict(X_test).flatten()\n",
    "                    cv_rmses.append(math.sqrt(mean_squared_error(y_ts, y_pred)))\n",
    "\n",
    "                avg_rmse = np.mean(cv_rmses)\n",
    "                if avg_rmse < best_rmse:\n",
    "                    best_rmse, best_cfg = avg_rmse, {\n",
    "                        \"units\":units, \"dropout\":drop,\n",
    "                        \"lr\":lr, \"batch_size\":bs\n",
    "                    }\n",
    "\n",
    "print(\"\\nBest CV‑RMSE (Log‑Renditen):\", best_rmse)\n",
    "print(\"Best Config:\", best_cfg)\n",
    "\n",
    "# 6) Endgültiges Training & pro-Fold‑RMSEs ausgeben\n",
    "fold_rmse_ret = []\n",
    "fold_rmse_prc = []\n",
    "fold_results  = []\n",
    "\n",
    "for fold, (tr_idx, te_idx) in enumerate(tscv.split(df), 1):\n",
    "    train_df = df.iloc[tr_idx]\n",
    "    test_df  = df.iloc[te_idx]\n",
    "    X_tr, y_tr = make_xy(train_df)\n",
    "    X_ts, y_ts = make_xy(test_df)\n",
    "\n",
    "    # reshape\n",
    "    X_tr = X_tr.reshape((-1, X_tr.shape[1],1))\n",
    "    X_ts = X_ts.reshape((-1, X_ts.shape[1],1))\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_tr.shape[1],1)),\n",
    "        LSTM(best_cfg[\"units\"], return_sequences=True),\n",
    "        Dropout(best_cfg[\"dropout\"]),\n",
    "        LSTM(best_cfg[\"units\"]),\n",
    "        Dropout(best_cfg[\"dropout\"]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(best_cfg[\"lr\"]), loss=\"mse\")\n",
    "    es = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "    model.fit(X_tr, y_tr, epochs=20,\n",
    "              batch_size=best_cfg[\"batch_size\"],\n",
    "              callbacks=[es], verbose=0)\n",
    "\n",
    "    # Vorhersage\n",
    "    y_pred = model.predict(X_ts).flatten()\n",
    "    rm_ret = math.sqrt(mean_squared_error(y_ts, y_pred))\n",
    "    fold_rmse_ret.append(rm_ret)\n",
    "\n",
    "    # Rückrechnung Kurse\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        p0 = test_df[\"Close\"].iloc[i+window_size-1]\n",
    "        preds.append(p0 * np.exp(r))\n",
    "        actuals.append(test_df[\"Close\"].iloc[i+window_size])\n",
    "    rm_prc = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    fold_rmse_prc.append(rm_prc)\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE Log‐Renditen = {rm_ret:.4f}, RMSE Preise = {rm_prc:.4f}\")\n",
    "\n",
    "    fold_results.append({\n",
    "        \"idx\":      test_df.index[window_size:],\n",
    "        \"y_test\":   y_ts,\n",
    "        \"y_pred\":   y_pred,\n",
    "        \"actuals\":  actuals,\n",
    "        \"preds\":    preds\n",
    "    })\n",
    "\n",
    "# Durchschnitt über alle Folds\n",
    "print(f\"\\nDurchschn. RMSE Log‑Renditen: {np.mean(fold_rmse_ret):.4f}\")\n",
    "print(f\"Durchschn. RMSE Aktienkurse: {np.mean(fold_rmse_prc):.4f}\")\n",
    "\n",
    "# 7) Plots: alle Folds\n",
    "plt.figure(figsize=(12,5))\n",
    "for i, fr in enumerate(fold_results, 1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_test\"],  label=f\"Real Fold {i}\", alpha=0.8)\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_pred\"],  \"--\", label=f\"Pred Fold {i}\", alpha=0.8)\n",
    "plt.title(f\"{ticker} – Log‑Renditen alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log‑Rendite\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "for i, fr in enumerate(fold_results, 1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"actuals\"], label=f\"Real Fold {i}\", alpha=0.8)\n",
    "    plt.plot(fr[\"idx\"], fr[\"preds\"],   \"--\", label=f\"Pred Fold {i}\", alpha=0.8)\n",
    "plt.title(f\"{ticker} – Aktienkurse alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ],
   "id": "29dba00fc5a0fa39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GARCH_XGBoost_RSI_GTD.py",
   "id": "c91acb04dd941a17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Einstellungen\n",
    "ticker      = \"NVDA\"\n",
    "window_size = 10\n",
    "n_splits    = 3\n",
    "csv_path    = f\"../../03_Daten/processed_data/merged_weekly_{ticker}_2015-2025.csv\"\n",
    "\n",
    "# 1) Daten einlesen und Log-Renditen berechnen\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"Date\"], index_col=\"Date\").sort_index()\n",
    "df[\"Return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 2) GARCH auf gesamten Datensatz einmalig fitten und Volatilität speichern\n",
    "g      = arch_model(df[\"Return\"] * 10, mean=\"Zero\", vol=\"GARCH\", p=1, q=1,\n",
    "                    dist=\"normal\", rescale=False)\n",
    "res_all = g.fit(disp=\"off\")\n",
    "df[\"GARCH_vol\"] = res_all.conditional_volatility / 10\n",
    "\n",
    "# 3) Statische Feature-Spalten ermitteln und skalieren\n",
    "gtd_cols    = [c for c in df.columns if \"stock\" in c.lower()]\n",
    "static_cols = [\"GARCH_vol\", \"RSI_14\"] + gtd_cols\n",
    "print(\"Verwendete statische Features (GTD + RSI + GARCH):\", static_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[static_cols] = scaler.fit_transform(df[static_cols])\n",
    "\n",
    "# 4) Funktion zum Erzeugen von X, y\n",
    "def make_xy(subdf):\n",
    "    X, y = [], []\n",
    "    rets = subdf[\"Return\"].values\n",
    "    for i in range(window_size, len(subdf)):\n",
    "        seq  = list(rets[i-window_size:i])                   # letzte Log-Renditen\n",
    "        stat = subdf[static_cols].iloc[i].values.tolist()     # GARCH_vol, RSI, GTD\n",
    "        X.append(seq + stat)\n",
    "        y.append(rets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 5) Hyperparameter-Suche für XGBoost\n",
    "X_full, y_full = make_xy(df)\n",
    "\n",
    "tscv_search = TimeSeriesSplit(n_splits=n_splits)\n",
    "param_dist = {\n",
    "    \"n_estimators\":     [50, 100, 200],\n",
    "    \"max_depth\":        [3, 5, 7],\n",
    "    \"learning_rate\":    [0.01, 0.05, 0.1],\n",
    "    \"subsample\":        [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "    \"gamma\":            [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# tree_method=\"hist\" für CPU-Training, n_jobs=1 um GPU-Parallelität auszuschließen\n",
    "xgb = XGBRegressor(\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_dist,\n",
    "    n_iter=20,\n",
    "    cv=tscv_search,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=1,            # nur ein Job, um CUDA-Konflikte zu vermeiden\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "search.fit(X_full, y_full)\n",
    "best_params = search.best_params_\n",
    "print(\">>> Best XGBoost Params:\", best_params)\n",
    "\n",
    "# 6) Out-of-Sample-Evaluation & Metriken\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "rmse_ret, mae_ret, mape_ret, r2_ret = [], [], [], []\n",
    "hit_ret, sharpe_ret               = [], []\n",
    "rmse_prc, mae_prc, mape_prc, r2_prc = [], [], [], []\n",
    "hit_prc, sharpe_prc               = [], []\n",
    "\n",
    "fold_results = []  # für Plots aller Folds\n",
    "\n",
    "for fold, (tr_idx, te_idx) in enumerate(tscv.split(df), start=1):\n",
    "    train_df = df.iloc[tr_idx].copy()\n",
    "    test_df  = df.iloc[te_idx].copy()\n",
    "\n",
    "    # a) GARCH-Forecast für Test-Set\n",
    "    fc = res_all.forecast(start=train_df.index[-1], horizon=len(test_df), reindex=False)\n",
    "    test_df[\"GARCH_vol\"] = np.sqrt(fc.variance.values[-1, :]) / 10\n",
    "\n",
    "    # b) Skalierung der statischen Features\n",
    "    train_df[static_cols] = scaler.transform(train_df[static_cols])\n",
    "    test_df[static_cols]  = scaler.transform(test_df[static_cols])\n",
    "\n",
    "    # c) Features/Targets\n",
    "    X_train, y_train = make_xy(train_df)\n",
    "    X_test,  y_test  = make_xy(test_df)\n",
    "    print(f\"Fold {fold}: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # d) Model trainieren mit Early Stopping\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=1,\n",
    "        early_stopping_rounds=10,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # e) Log-Return Metriken\n",
    "    y_pred = model.predict(X_test)\n",
    "    rm = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    ma = mean_absolute_error(y_test, y_pred)\n",
    "    mask_r = y_test != 0\n",
    "    mp = np.mean(np.abs((y_test[mask_r] - y_pred[mask_r]) / y_test[mask_r])) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    dir_true = np.sign(np.diff(y_test))\n",
    "    dir_pred = np.sign(np.diff(y_pred))\n",
    "    hr = (dir_true == dir_pred).mean() * 100\n",
    "    rr = np.diff(y_pred) / y_pred[:-1]\n",
    "    sr = rr.mean() / (rr.std() if rr.std() != 0 else np.nan)\n",
    "\n",
    "    rmse_ret.append(rm); mae_ret.append(ma)\n",
    "    mape_ret.append(mp); r2_ret.append(r2)\n",
    "    hit_ret.append(hr); sharpe_ret.append(sr)\n",
    "\n",
    "    # f) Preis-Metriken\n",
    "    preds, actuals = [], []\n",
    "    for i, r in enumerate(y_pred):\n",
    "        p0 = test_df[\"Close\"].iat[i + window_size - 1]\n",
    "        preds.append(p0 * np.exp(r))\n",
    "        actuals.append(test_df[\"Close\"].iat[i + window_size])\n",
    "    preds   = np.array(preds)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    rm_p  = math.sqrt(mean_squared_error(actuals, preds))\n",
    "    ma_p  = mean_absolute_error(actuals, preds)\n",
    "    mask_p = actuals != 0\n",
    "    mp_p = np.mean(np.abs((actuals[mask_p] - preds[mask_p]) / actuals[mask_p])) * 100\n",
    "    r2_p = r2_score(actuals, preds)\n",
    "    dir_tp = np.sign(np.diff(actuals))\n",
    "    dir_pp = np.sign(np.diff(preds))\n",
    "    hr_p = (dir_tp == dir_pp).mean() * 100\n",
    "    rp = np.diff(preds) / preds[:-1]\n",
    "    sr_p = rp.mean() / (rp.std() if rp.std() != 0 else np.nan)\n",
    "\n",
    "    rmse_prc.append(rm_p); mae_prc.append(ma_p)\n",
    "    mape_prc.append(mp_p); r2_prc.append(r2_p)\n",
    "    hit_prc.append(hr_p); sharpe_prc.append(sr_p)\n",
    "\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"  Returns → RMSE={rm:.4f}, MAE={ma:.4f}, MAPE={mp:.2f}%, \"\n",
    "          f\"R²={r2:.4f}, Hit-Rate={hr:.1f}%, Sharpe={sr:.4f}\")\n",
    "    print(f\"  Prices  → RMSE={rm_p:.4f}, MAE={ma_p:.4f}, MAPE={mp_p:.2f}%, \"\n",
    "          f\"R²={r2_p:.4f}, Hit-Rate={hr_p:.1f}%, Sharpe={sr_p:.4f}\\n\")\n",
    "\n",
    "    # Ergebnisse für spätere Plots speichern\n",
    "    fold_results.append({\n",
    "        \"idx\":    test_df.index[window_size:],\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"actual\": actuals,\n",
    "        \"preds\":  preds\n",
    "    })\n",
    "\n",
    "# 7) Durchschnittsergebnisse ausgeben\n",
    "print(\"\\n=== Durchschnittliche Metriken: Log-Renditen ===\")\n",
    "print(f\"RMSE    = {np.mean(rmse_ret):.4f}\")\n",
    "print(f\"MAE     = {np.mean(mae_ret):.4f}\")\n",
    "print(f\"MAPE    = {np.mean(mape_ret):.2f}%\")\n",
    "print(f\"R²      = {np.mean(r2_ret):.4f}\")\n",
    "print(f\"HitRate = {np.mean(hit_ret):.2f}%\")\n",
    "print(f\"Sharpe  = {np.nanmean(sharpe_ret):.4f}\")\n",
    "\n",
    "print(\"\\n=== Durchschnittliche Metriken: Preise ===\")\n",
    "print(f\"RMSE    = {np.mean(rmse_prc):.4f}\")\n",
    "print(f\"MAE     = {np.mean(mae_prc):.4f}\")\n",
    "print(f\"MAPE    = {np.mean(mape_prc):.2f}%\")\n",
    "print(f\"R²      = {np.mean(r2_prc):.4f}\")\n",
    "print(f\"HitRate = {np.mean(hit_prc):.2f}%\")\n",
    "print(f\"Sharpe  = {np.nanmean(sharpe_prc):.4f}\")\n",
    "\n",
    "# 8) Plots für alle Folds\n",
    "\n",
    "# a) Log-Renditen über alle Folds\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, fr in enumerate(fold_results, start=1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_test\"],  label=f\"Real Ret Fold {i}\",  alpha=0.7)\n",
    "    plt.plot(fr[\"idx\"], fr[\"y_pred\"], \"--\",               label=f\"Pred Ret Fold {i}\", alpha=0.7)\n",
    "plt.title(f\"{ticker} – Log-Renditen über alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Log-Rendite\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# b) Close-Preise über alle Folds\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, fr in enumerate(fold_results, start=1):\n",
    "    plt.plot(fr[\"idx\"], fr[\"actual\"], label=f\"Real Price Fold {i}\", alpha=0.7)\n",
    "    plt.plot(fr[\"idx\"], fr[\"preds\"],  \"--\",               label=f\"Pred Price Fold {i}\", alpha=0.7)\n",
    "plt.title(f\"{ticker} – Close-Preise über alle {n_splits} Folds\")\n",
    "plt.xlabel(\"Datum\"); plt.ylabel(\"Preis (Close)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ],
   "id": "ac09ccf9c4152f01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisierungen",
   "id": "a6ec09e3d2645468"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vis.py",
   "id": "c0af1c1174103daf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modellvarianten\n",
    "models = [\"LSTM\", \"XGBoost\", \"G+L\", \"G+L+RSI\", \"G+L+RSI+GTD\", \"G+X\", \"G+X+RSI\", \"G+X+RSI+GTD\"]\n",
    "x = range(len(models))\n",
    "\n",
    "# RMSE-Werte Preisprognose (Schlusskurse)\n",
    "rmse_nvda = [6.6339, 27.0426, 2.7375, 2.3375, 2.8136, 2.8578, 2.4853, 2.4625]\n",
    "rmse_goog = [9.4500, 30.0505, 3.9578, 3.5819, 6.2158, 4.4258, 3.7953, 3.6881]\n",
    "rmse_msft = [18.9792, 80.4302, 7.6404, 6.9672, 14.3669, 8.0835, 7.2955, 7.1738]\n",
    "\n",
    "# Visualisierung 1: Reine ML-Modelle vs. GARCH-Hybride\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x, rmse_nvda, color='green', label=\"NVDA\", marker='o')\n",
    "plt.scatter(x, rmse_goog, color='blue', label=\"GOOG\", marker='s')\n",
    "plt.scatter(x, rmse_msft, color='orange', label=\"MSFT\", marker='^')\n",
    "plt.title(\"RMSE – Reine ML-Modelle vs. GARCH-Hybride (Preisprognose)\")\n",
    "plt.xlabel(\"Modellvariante\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(x, models, rotation=30)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Modellvarianten der GARCH-Hybride\n",
    "models_hybrid = [\"G+L\", \"G+L+RSI\", \"G+L+RSI+GTD\", \"G+X\", \"G+X+RSI\", \"G+X+RSI+GTD\"]\n",
    "x_hybrid = range(len(models_hybrid))\n",
    "\n",
    "# RMSE-Werte Preisprognose aus den Tabellen\n",
    "rmse_nvda = [2.7375, 2.3375, 2.8136, 2.8578, 2.4853, 2.4625]\n",
    "rmse_goog = [3.9578, 3.5819, 6.2158, 4.4258, 3.7953, 3.6881]\n",
    "rmse_msft = [7.6404, 6.9672, 14.3669, 8.0835, 7.2955, 7.1738]\n",
    "\n",
    "# Diagramm mit Punkten\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_hybrid, rmse_nvda, color='green', label=\"NVDA\", marker='o')\n",
    "plt.scatter(x_hybrid, rmse_goog, color='blue', label=\"GOOG\", marker='s')\n",
    "plt.scatter(x_hybrid, rmse_msft, color='orange', label=\"MSFT\", marker='^')\n",
    "\n",
    "# Achsenbeschriftungen und Formatierung\n",
    "plt.title(\"RMSE – Preisprognose: Einfluss von Features auf GARCH-Hybridmodelle\")\n",
    "plt.xlabel(\"Modellvariante\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(x_hybrid, models_hybrid, rotation=30)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "964eba1060b4835d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
